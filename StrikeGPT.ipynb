{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOPkgvsVFnEg0omHv5jMtek",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j0rdan0/AI-notebooks/blob/main/StrikeGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers unsloth"
      ],
      "metadata": {
        "id": "NLcfEuN-AZd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaOoLoSc_bab"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "\n",
        "max_seq_length = 10*4096 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Bouquets/StrikeGPT-R1-Zero-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\",\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LDlRRX1A_w_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"write a simple exploit for buffer overflow to get a shell.Give me the exploit code in python\"\n",
        "input_text = 'the source code of the binary to exploit \\\n",
        " #include <stdio.h>\\\n",
        "int main(int argc, char **argv) { \\\n",
        "  char buf[8];         // buffer for eight characters \\\n",
        "  gets(buf);           // read from stdio (sensitive function!) \\\n",
        "  printf(\"%s\\n\", buf); // print out data stored in buf \\\n",
        "  return 0;            // 0 as return value \\\n",
        "} '"
      ],
      "metadata": {
        "id": "ME3OMbMsAUVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ],
      "metadata": {
        "id": "jdsCd9oECTvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        instruction, # instruction\n",
        "        input_text, # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "0_IUOZGz_xXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(**inputs,streamer = text_streamer, max_new_tokens = 8192, pad_token_id = tokenizer.eos_token_id)\n"
      ],
      "metadata": {
        "id": "SNTDcS0x_4AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XNLzPcXaCZo8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}