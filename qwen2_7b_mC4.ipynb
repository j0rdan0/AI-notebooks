{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDMLnWTOGABLts49fAULiw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j0rdan0/AI-notebooks/blob/main/qwen2_7b_mC4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5zpCdEfItH7"
      },
      "outputs": [],
      "source": [
        "#!pip install huggingface_hub\n",
        "#!pip install  BitsAndBytes\n",
        "#!pip install peft # for LoRA\n",
        "#!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hf_auth():\n",
        "  import huggingface_hub\n",
        "  from google.colab import userdata\n",
        "\n",
        "  hf_token = userdata.get('HF_TOKEN')\n",
        "  huggingface_hub.login(token=hf_token)"
      ],
      "metadata": {
        "id": "D3Bn6tpTI3HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: train an small LM with https://huggingface.co/datasets/uonlp/CulturaX for RO language, using LoRA\n",
        "\n",
        "model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
        "\n",
        "def generate_base_model(model_name):\n",
        "  from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "  from accelerate.test_utils.testing import get_backend\n",
        "\n",
        "  device,_,_ = get_backend()\n",
        "  hf_auth()\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name,device_map=\"auto\",attn_implementation=\"flash_attention_2\",torch_dtype=\"auto\").to(device)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "CHhL08vsJPcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, TaskType,get_peft_model\n",
        "\n",
        "def generate_peft_model(model):\n",
        "  peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
        "  return get_peft_model(model,peft_config)"
      ],
      "metadata": {
        "id": "euj0DS6-C3zU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_tokenizer(model_name):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name,padding_side=\"left\")"
      ],
      "metadata": {
        "id": "PT3qV_2kCuMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_pretrained_model(model,model_name):\n",
        "  merged_model = model.merge_and_unload()\n",
        "  merged_model.save_pretrained(model_name)\n",
        ""
      ],
      "metadata": {
        "id": "-W5DbO_wC4hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_dataset_rm(dataset,tokenizer):\n",
        "    return dataset.map(lambda sample: tokenizer(sample[\"text\"],truncation=True,padding='max_length'),batched=True,remove_columns=sample.column_names) # we dont need any columns anymore\n"
      ],
      "metadata": {
        "id": "z9WVD0DTG-t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset import load_dataset\n",
        "\n",
        "def process_dataset(tokenizer):\n",
        "  dataset_name = (\"uonlp/CulturaX\",\"ro\")\n",
        "  dataset = load_dataset(dataset_name[0],dataset_name[1],streaming=True,split=\"train\")\n",
        "  return tokenize_dataset_rm(dataset,tokenizer)"
      ],
      "metadata": {
        "id": "7pqmbDzKF9v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "def generate_data_collator(tokenizer):\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  return DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "8AsVAQdRHiC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_trainer(model,train_dataset,data_collator,tokenizer):\n",
        "  training_args = TrainingArguments(\n",
        "    output_dir=\"qwen2_7B_mC4_ro\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "  trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=lm_dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "  return trainer\n"
      ],
      "metadata": {
        "id": "pjfBLqfdJVmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def push_model(trainer):\n",
        "  trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "O9ZybIjrMHlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
        "\n",
        "def main():\n",
        "  hf_auth()\n",
        "  model = generate_base_model(model_name)\n",
        "  model = generate_peft_model(model)\n",
        "  tokenizer = generate_tokenizer(model_name)\n",
        "  dataset = process_dataset(tokenizer)\n",
        "  data_collator = generate_data_collator(tokenizer)\n",
        "\n",
        "  trainer = generate_trainer(model,dataset,data_collator,tokenizer)\n",
        "\n",
        "  trainer.train()\n",
        "  #push_model(trainer)\n"
      ],
      "metadata": {
        "id": "L6saoam1Ke7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZKyQmpaoLbzM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}